{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import gzip\n",
    "import bcolz\n",
    "import numpy as np\n",
    "import re\n",
    "import copy\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the dataset in 9.44 seconds.\n"
     ]
    }
   ],
   "source": [
    "pickleFile = '../Datasets/Reviews/dataset.pkl'\n",
    "start = time.clock()\n",
    "dataset = pickle.load( open( pickleFile, \"rb\" ))\n",
    "duration = time.clock() - start\n",
    "print(\"Loaded the dataset in\", round(duration,2), \"seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 63001 amazon items.\n"
     ]
    }
   ],
   "source": [
    "print(\"There are\",len(dataset), \"amazon items.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Glove Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preloading files.. Done.\n"
     ]
    }
   ],
   "source": [
    "def loadGlove(glove_path, dim=50):\n",
    "    acceptedDimensions = [50, 100, 200, 300]\n",
    "    if dim not in acceptedDimensions:\n",
    "        print(\"You didn't choose a right dimension.\")\n",
    "        print(\"Try one of these:\", acceptedDimensions)\n",
    "        return None\n",
    "    pickleWordFile = f'{glove_path}/6B.'+str(dim)+'_words.pkl'\n",
    "    pickleIdFile   = f'{glove_path}/6B.'+str(dim)+'_idx.pkl'\n",
    "    pickleDatFile  = f'{glove_path}/glove.6B.'+str(dim)+'.dat'\n",
    "    pickleDataset  = f'{glove_path}/glove.6B.'+str(dim)+'d.txt'\n",
    "    \n",
    "    if os.path.isfile(pickleWordFile):\n",
    "        # check if we've made the outputs before\n",
    "        print(\"Preloading files..\", end=\" \")\n",
    "        vectors = bcolz.open(pickleDatFile)[:]\n",
    "        words = pickle.load(open(pickleWordFile, 'rb'))\n",
    "        word2idx = pickle.load(open(pickleIdFile, 'rb'))\n",
    "        glove = {w: vectors[word2idx[w]] for w in words}\n",
    "        print(\"Done.\")\n",
    "        return glove\n",
    "    else:\n",
    "        print(\"Doesn't work.\", end=\" \")\n",
    "\n",
    "\n",
    "gloveDimension = 50\n",
    "glovePath = \"/media/data/Datasets/glove\"\n",
    "glove = loadGlove(glovePath, dim=gloveDimension)\n",
    "gloveWords = glove.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glove Vocabulary Size: 400000\n"
     ]
    }
   ],
   "source": [
    "print(\"Glove Vocabulary Size:\",len(gloveWords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(paragraph):\n",
    "    # split paragraph by full stops\n",
    "    paragraph = paragraph.lower()\n",
    "    paragraph = re.sub(\"([,!?()-+&£$.%*'])\", r' \\1 ', paragraph)\n",
    "    paragraph = re.sub('\\s{2,}', ' ', paragraph)\n",
    "    paragraph = paragraph.split(\" \")\n",
    "    # remove empty string\n",
    "    return paragraph\n",
    "\n",
    "def padSentence(words, maxLength, eosString=\"<eos>\", padString=\"<pad>\"):\n",
    "    words = words[:maxLength-1] + [eosString]\n",
    "    return words + [padString for i in range(maxLength - len(words))]\n",
    "    \n",
    "def discretise(value, word):\n",
    "    return word + \"_\" + str(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handleItem(itemID, dataset=dataset, printDebug=False):\n",
    "    \"\"\"\n",
    "    Filters words out based on whether they're in the GloVe dataset or not.\n",
    "    \n",
    "    Parameters:\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    reviews = []\n",
    "    for i in range(len(dataset[itemID])):\n",
    "        # initialise variables\n",
    "        entry = dataset[itemID][i]\n",
    "        reviewerID = entry['reviewerID']\n",
    "\n",
    "        \"\"\"\n",
    "        Review Text Processing\n",
    "        \"\"\"\n",
    "        # split sentences\n",
    "        sentences = re.split(r'(?<=\\.) ', entry['reviewText'])\n",
    "        for s in range(len(sentences)):\n",
    "            sentence = sentences[s]\n",
    "            sentences[s] = list(filter(None, preprocess(sentence)))\n",
    "            \n",
    "        # preprocess summary\n",
    "        summary = list(filter(None,preprocess(entry['summary'])))\n",
    "        \n",
    "        # merge summary sequence and review sequences together into overall entries.\n",
    "        if len(sentences) < 2:\n",
    "            entries =  [[\"<sos>\"] + summary + [\"<sor>\"]] + [sentences[0] + [\"<eos>\"]]\n",
    "        else:\n",
    "            entries =  [[\"<sos>\"] + summary + [\"<sor>\"]] + [sentences[0]] + sentences[1:-1] + [sentences[-1] + [\"<eos>\"]]\n",
    "\n",
    "        # setup review parameters\n",
    "        rating   = [discretise(entry['overall'], \"rating\")]\n",
    "\n",
    "        # compute polarity\n",
    "        good, bad = entry['helpful'][0], entry['helpful'][1]\n",
    "        try:\n",
    "            polarity = (good - bad) / (good + bad)\n",
    "        except ZeroDivisionError:\n",
    "            polarity = 0\n",
    "        polarity = np.tanh(polarity)\n",
    "        polarity = np.round(polarity, 1)\n",
    "        polarity = [discretise(polarity, \"polarity\")]\n",
    "\n",
    "        # create identity/conditioning entry\n",
    "        identifier = itemID.lower()\n",
    "        identity = [l for l in identifier] + rating + polarity\n",
    "\n",
    "        # add conditionining entry to each entry\n",
    "        formatted = [entry for entry in entries]\n",
    "\n",
    "\n",
    "        if printDebug:\n",
    "            print(\"ENTRY:\",dataset[itemID][i])\n",
    "            print(\"IDENTITY:\",identity)\n",
    "\n",
    "\n",
    "        for i in range(len(formatted)-1):\n",
    "            # add the conditioning variable to the input. the output value does not have the conditioning variable.\n",
    "            reviews.append([identity + formatted[i], formatted[i+1]])\n",
    "            if printDebug:\n",
    "                print(reviews[-1][0], \"->\", reviews[-1][1])\n",
    "        if printDebug:\n",
    "            break\n",
    "            \n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTRY: {'reviewerID': 'AA8JH8LD2H4P9', 'asin': '7214047977', 'reviewerName': 'Claudia J. Frier', 'helpful': [3, 4], 'reviewText': 'This fits my 7\" kindle fire hd perfectly! I love it. It even has a slot for a stylus. The kindle is velcroed in so it\\'s nice and secure. Very glad I bought this!', 'overall': 5.0, 'summary': 'love it', 'unixReviewTime': 1354665600, 'reviewTime': '12 5, 2012'}\n",
      "IDENTITY: ['7', '2', '1', '4', '0', '4', '7', '9', '7', '7', 'rating_5.0', 'polarity_-0.1']\n",
      "['7', '2', '1', '4', '0', '4', '7', '9', '7', '7', 'rating_5.0', 'polarity_-0.1', '<sos>', 'love', 'it', '<sor>'] -> ['this', 'fits', 'my', '7\"', 'kindle', 'fire', 'hd', 'perfectly', '!', 'i', 'love', 'it', '.']\n",
      "['7', '2', '1', '4', '0', '4', '7', '9', '7', '7', 'rating_5.0', 'polarity_-0.1', 'this', 'fits', 'my', '7\"', 'kindle', 'fire', 'hd', 'perfectly', '!', 'i', 'love', 'it', '.'] -> ['it', 'even', 'has', 'a', 'slot', 'for', 'a', 'stylus', '.']\n",
      "['7', '2', '1', '4', '0', '4', '7', '9', '7', '7', 'rating_5.0', 'polarity_-0.1', 'it', 'even', 'has', 'a', 'slot', 'for', 'a', 'stylus', '.'] -> ['the', 'kindle', 'is', 'velcroed', 'in', 'so', 'it', \"'\", 's', 'nice', 'and', 'secure', '.']\n",
      "['7', '2', '1', '4', '0', '4', '7', '9', '7', '7', 'rating_5.0', 'polarity_-0.1', 'the', 'kindle', 'is', 'velcroed', 'in', 'so', 'it', \"'\", 's', 'nice', 'and', 'secure', '.'] -> ['very', 'glad', 'i', 'bought', 'this', '!', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "datasetKeys = list(dataset.keys())\n",
    "k = handleItem(datasetKeys[19],printDebug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63001\n"
     ]
    }
   ],
   "source": [
    "print(len(datasetKeys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processItems(func, args, n_processes = 7):\n",
    "    p = Pool(n_processes)\n",
    "    res_list = []\n",
    "    with tqdm(total = len(args)) as pbar:\n",
    "        for i, res in enumerate(p.imap_unordered(func, args)):\n",
    "            pbar.update()\n",
    "            res_list.append(res)\n",
    "    pbar.close()\n",
    "    p.close()\n",
    "    p.join()\n",
    "    return res_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3938/3938 [00:06<00:00, 620.40it/s]\n"
     ]
    }
   ],
   "source": [
    "reviews = processItems(handleItem,datasetKeys[::16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3150 788\n",
      "498155 122831\n"
     ]
    }
   ],
   "source": [
    "datasetSize = len(reviews)\n",
    "trainPortion = 0.8\n",
    "valPortion = 0.2\n",
    "\n",
    "trainRatio = int(datasetSize * trainPortion)\n",
    "valRatio = int(datasetSize * valPortion)\n",
    "\n",
    "train = reviews[:trainRatio]\n",
    "validation = reviews[trainRatio:]\n",
    "\n",
    "print(len(train), len(validation))\n",
    "\n",
    "# now we need to flatten train and validation.\n",
    "trainents = []\n",
    "for review in train:\n",
    "    trainents += [entry for entry in review]\n",
    "valents = []\n",
    "for review in validation:\n",
    "    valents += [entry for entry in review]\n",
    "    \n",
    "train = trainents\n",
    "validation = valents\n",
    "\n",
    "print(len(train), len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3', '9', '3', '0', '9', '9', '2', '8', '6', '8', 'rating_5.0', 'polarity_0.0', '<sos>', 'harddrive', '<sor>'], ['brought', 'back', 'my', 'ipod', 'video', '30g', 'back', 'life', ',', 'easy', 'to', 'replace', '.']]\n"
     ]
    }
   ],
   "source": [
    "# get the number of itemIDs\n",
    "for row in train:\n",
    "    print(row)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting ID's of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve the throughput of the model, we should reduce the embedding size. Here we'll look at all the words and keep track ones that exist. We'll make a reduced word2id based on this set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subsetting Reviews..\n",
      "We now have 498155 reviews.\n"
     ]
    }
   ],
   "source": [
    "# here we reduce the size of the dataset so we can debug our model.\n",
    "print(\"Subsetting Reviews..\")\n",
    "print(\"We now have\", len(train), \"reviews.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498155/498155 [00:08<00:00, 55847.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# get word frequency for words in training data\n",
    "for row in tqdm(train):\n",
    "    for sequences in row:\n",
    "        for word in sequences:\n",
    "            word = str(word)\n",
    "            if word not in wordcounts:\n",
    "                wordcounts[word] = 0\n",
    "            wordcounts[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get words that are not in the glove dataset\n",
    "knowns   = [word for word in wordcounts if word in glove]\n",
    "unknowns = [word for word in wordcounts if word not in glove]\n",
    "# sort words by their frequency\n",
    "wordOrder = list(sorted(knowns, key=lambda x: wordcounts[x], reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43023 96140\n"
     ]
    }
   ],
   "source": [
    "print(len(knowns), len(unknowns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabularyLimit = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordOrder = wordOrder[:vocabularyLimit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [glove[word] for word in wordOrder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in unknowns:\n",
    "    if (\"rating\" in word) or (\"polarity\" in word):\n",
    "        try:\n",
    "            part = word.split(\"_\")\n",
    "            if part[1] == \"-0.0\":\n",
    "                part[1] = \"0.0\"\n",
    "            weight = glove[part[0]] + glove[part[1]]\n",
    "            wordOrder.append(word)\n",
    "            weights.append(weight)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionaries for constant time referencing\n",
    "id2word = {idx: w for (idx, w) in enumerate(wordOrder)}\n",
    "word2id = {w: idx for (idx, w) in enumerate(wordOrder)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lim = len(word2id)\n",
    "# add <eos> (end of sequence)\n",
    "weights.append(glove['eos'])\n",
    "word2id['<eos>'] = lim\n",
    "id2word[lim] = '<eos>'\n",
    "lim += 1\n",
    "\n",
    "# add <sos> (start of sequence)\n",
    "weights.append(glove['sos'])\n",
    "word2id['<sos>'] = lim\n",
    "id2word[lim] = ['<sos>']\n",
    "lim += 1\n",
    "\n",
    "# add <sor> (start of review)\n",
    "sorWeight = np.random.normal(0,0.5,gloveDimension)\n",
    "weights.append(sorWeight)\n",
    "word2id['<sor>'] = lim\n",
    "id2word[lim] = '<sor>'\n",
    "lim += 1\n",
    "\n",
    "# add <unk> (unknown token)\n",
    "weights.append(glove['unk'])\n",
    "word2id['<unk>'] = lim\n",
    "id2word[lim] = '<unk>'\n",
    "\n",
    "# add <pad> \n",
    "id2word[len(word2id)] = \"<pad>\"\n",
    "word2id[\"<pad>\"] = len(word2id)\n",
    "weights.append(np.random.normal(0,0,gloveDimension))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.normal(0,0,gloveDimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToID(word,corp=word2id):\n",
    "    if word in corp:\n",
    "        return corp[word]\n",
    "    return corp['<unk>']\n",
    "\n",
    "def IDToWord(id,corp=id2word, ref=word2id):\n",
    "    if word in corp:\n",
    "        return corp[word]\n",
    "    return corp[ref['<unk>']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498155/498155 [00:06<00:00, 81736.40it/s] \n",
      "100%|██████████| 122831/122831 [00:01<00:00, 98194.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# convert words to their id's in the review.\n",
    "def entriesToWordIDs(group):\n",
    "    return [[[wordToID(word) for word in seq] for seq in row] for row in tqdm(group)]\n",
    "    \n",
    "train = entriesToWordIDs(train)\n",
    "validation = entriesToWordIDs(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sequence in our dataset is 1697 tokens long.\n"
     ]
    }
   ],
   "source": [
    "sizes = {}\n",
    "for row in train:\n",
    "    for seq in row:\n",
    "        length = len(seq)\n",
    "        if length not in sizes:\n",
    "            sizes[length] = 0\n",
    "        sizes[length] += 1\n",
    "\n",
    "seqlengths = list(sorted(sizes.keys(), key=lambda x: sizes[x], reverse=True))\n",
    "print(\"The longest sequence in our dataset is\",max(seqlengths),\"tokens long.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1709):\n",
    "    if i not in sizes:\n",
    "        sizes[i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ents = [x for x in range(0,70)]\n",
    "bins = [sizes[x] for x in ents]\n",
    "plt.bar(ents,bins)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cutoff sequence length: 60\n"
     ]
    }
   ],
   "source": [
    "cutoff = 60\n",
    "print(\"Cutoff sequence length:\", cutoff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 498155/498155 [00:02<00:00, 220215.29it/s]\n",
      "100%|██████████| 122831/122831 [00:00<00:00, 684247.61it/s]\n"
     ]
    }
   ],
   "source": [
    "def trimSeq(group):\n",
    "    return [[seq[:cutoff] for seq in row] for row in tqdm(group)]\n",
    "\n",
    "train = trimSeq(train)\n",
    "validation = trimSeq(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create container ready for use in dataset\n",
    "# we do not add padding here as we want to reduce storage size!\n",
    "container = {\n",
    "    'id2word' : id2word,\n",
    "    'word2id' : word2id,\n",
    "    'train' : train,\n",
    "    'validation': validation,\n",
    "    'weights' : np.matrix(weights),\n",
    "    'cutoff' : cutoff\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved! 98.3 MB\n"
     ]
    }
   ],
   "source": [
    "def convert_bytes(num):\n",
    "    \"\"\"\n",
    "    this function will convert bytes to MB.... GB... etc\n",
    "    \"\"\"\n",
    "    for x in ['bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num /= 1024.0\n",
    "        \n",
    "        \n",
    "datasetFile = '../Datasets/Reviews/dataset_ready.pkl'\n",
    "# save the dataset to a pickle file.\n",
    "output = open(datasetFile, 'wb')\n",
    "pickle.dump(container, output)\n",
    "output.close()\n",
    "print(\"Saved!\", convert_bytes(os.stat(datasetFile).st_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train[0][::32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sequence lengths for train and validation data\n",
    "trainx = [x[0] for x in train]\n",
    "trainy = [x[1] for x in train]\n",
    "valx   = [x[0] for x in validation]\n",
    "valy   = [x[1] for x in validation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Dataset batching mechanism\n",
    "\"\"\"\n",
    "\n",
    "def padSeq(row, maxlength, padID, cutoff):\n",
    "    currentLength = len(row)\n",
    "    difference = maxlength - currentLength\n",
    "    return row + [padID for _ in range(difference)]\n",
    "\n",
    "\n",
    "def batchData(dataset, padID, device, batchsize=32, cutoff=50):\n",
    "    \"\"\"\n",
    "    Splits the dataset into batches.\n",
    "    Each batch needs to be sorted by \n",
    "    the length of their sequence in order\n",
    "    for `pack_padded_sequence` to be used.\n",
    "    \"\"\"\n",
    "    datasize = len(dataset)\n",
    "    batches = []\n",
    "    # split data into batches.\n",
    "    for i in range(0, datasize, batchsize):\n",
    "        batches.append(dataset[i:i+batchsize])\n",
    "    # within each batch, sort the entries.\n",
    "    for i in range(len(batches)):\n",
    "        batch = batches[i]\n",
    "        # get lengths of each review in the batch\n",
    "        # based on the postion of the EOS tag.\n",
    "        lengths = [len(seq) for seq in batch]\n",
    "        indexes = [x for x in range(len(lengths))]\n",
    "        sortedindexes = sorted(list(zip(lengths, indexes)), reverse=True)\n",
    "\n",
    "        # since sentences are split by period, the period itself acts\n",
    "        # the token to identify that the sentence has ended.\n",
    "        # i.e. we don't need another token identifying the end of the subsequence.\n",
    "\n",
    "        # get the reviews based on the sorted batch lengths\n",
    "        reviews = [padSeq(batch[i[1]], cutoff, padID, cutoff)\n",
    "                   for i in sortedindexes]\n",
    "\n",
    "        reviews = torch.tensor(reviews, dtype=torch.long, device=device)\n",
    "        # re-allocate values.\n",
    "        batches[i] = (reviews, [i[0] for i in sortedindexes])\n",
    "    return batches\n",
    "device = \"cpu\"\n",
    "batchsize = 32\n",
    "cutoff = cutoff\n",
    "trainx_p = batchData(trainx, word2id['<pad>'], device, batchsize, cutoff)\n",
    "trainy_p = batchData(trainy, word2id['<pad>'], device, batchsize, cutoff)\n",
    "valx_p = batchData(valx, word2id['<pad>'], device, batchsize, cutoff)\n",
    "valy_p = batchData(valy, word2id['<pad>'], device, batchsize, cutoff)\n",
    "\n",
    "train_p = (trainx_p, trainy_p)\n",
    "val_p = (valx_p, valy_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30020, 50)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weightshape = np.matrix(weights).shape\n",
    "weightshape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 60, 1])\n",
      "tensor(0.9838)\n",
      "torch.Size([32, 59, 1])\n",
      "tensor(0.9835)\n",
      "torch.Size([32, 58, 1])\n",
      "tensor(0.9832)\n",
      "torch.Size([32, 57, 1])\n",
      "tensor(0.9836)\n",
      "torch.Size([32, 56, 1])\n",
      "tensor(0.9834)\n",
      "torch.Size([32, 55, 1])\n",
      "tensor(0.9835)\n",
      "torch.Size([32, 54, 1])\n",
      "tensor(0.9837)\n",
      "torch.Size([32, 53, 1])\n",
      "tensor(0.9835)\n",
      "torch.Size([32, 52, 1])\n",
      "tensor(0.9839)\n",
      "torch.Size([32, 51, 1])\n",
      "tensor(0.9837)\n",
      "torch.Size([32, 50, 1])\n",
      "tensor(0.9839)\n",
      "torch.Size([32, 49, 1])\n",
      "tensor(0.9836)\n",
      "torch.Size([32, 48, 1])\n",
      "tensor(0.9837)\n",
      "torch.Size([32, 47, 1])\n",
      "tensor(0.9839)\n",
      "torch.Size([32, 46, 1])\n",
      "tensor(0.9839)\n",
      "torch.Size([32, 45, 1])\n",
      "tensor(0.9835)\n",
      "torch.Size([32, 44, 1])\n",
      "tensor(0.9838)\n",
      "torch.Size([32, 43, 1])\n",
      "tensor(0.9836)\n",
      "torch.Size([32, 42, 1])\n",
      "tensor(0.9838)\n",
      "torch.Size([32, 41, 1])\n",
      "tensor(0.9837)\n",
      "torch.Size([32, 40, 1])\n",
      "tensor(0.9836)\n",
      "torch.Size([32, 39, 1])\n",
      "tensor(0.9841)\n",
      "torch.Size([32, 38, 1])\n",
      "tensor(0.9839)\n",
      "torch.Size([32, 37, 1])\n",
      "tensor(0.9837)\n",
      "torch.Size([32, 36, 1])\n",
      "tensor(0.9835)\n",
      "torch.Size([32, 35, 1])\n",
      "tensor(0.9840)\n",
      "torch.Size([32, 34, 1])\n",
      "tensor(0.9837)\n",
      "torch.Size([32, 33, 1])\n",
      "tensor(0.9838)\n",
      "torch.Size([32, 32, 1])\n",
      "tensor(0.9838)\n",
      "torch.Size([32, 31, 1])\n",
      "tensor(0.9840)\n",
      "torch.Size([32, 30, 1])\n",
      "tensor(0.9839)\n",
      "torch.Size([32, 29, 1])\n",
      "tensor(0.9839)\n",
      "torch.Size([32, 28, 1])\n",
      "tensor(0.9839)\n",
      "torch.Size([32, 27, 1])\n",
      "tensor(0.9839)\n",
      "torch.Size([32, 26, 1])\n",
      "tensor(0.9836)\n",
      "torch.Size([32, 25, 1])\n",
      "tensor(0.9839)\n",
      "torch.Size([32, 24, 1])\n",
      "tensor(0.9836)\n",
      "torch.Size([32, 23, 1])\n",
      "tensor(0.9836)\n",
      "torch.Size([32, 22, 1])\n",
      "tensor(0.9837)\n",
      "torch.Size([32, 21, 1])\n",
      "tensor(0.9840)\n",
      "torch.Size([32, 20, 1])\n",
      "tensor(0.9838)\n",
      "torch.Size([32, 19, 1])\n",
      "tensor(0.9839)\n",
      "torch.Size([32, 18, 1])\n",
      "tensor(0.9837)\n",
      "torch.Size([32, 17, 1])\n",
      "tensor(0.9839)\n",
      "torch.Size([32, 16, 1])\n",
      "tensor(0.9837)\n",
      "torch.Size([32, 15, 1])\n",
      "tensor(0.9838)\n",
      "torch.Size([32, 14, 1])\n",
      "tensor(0.9840)\n",
      "torch.Size([32, 13, 1])\n",
      "tensor(0.9837)\n",
      "torch.Size([32, 12, 1])\n",
      "tensor(0.9840)\n",
      "torch.Size([32, 11, 1])\n",
      "tensor(0.9838)\n",
      "torch.Size([32, 10, 1])\n",
      "tensor(0.9836)\n",
      "torch.Size([32, 9, 1])\n",
      "tensor(0.9835)\n",
      "torch.Size([32, 8, 1])\n",
      "tensor(0.9840)\n",
      "torch.Size([32, 7, 1])\n",
      "tensor(0.9838)\n",
      "torch.Size([32, 6, 1])\n",
      "tensor(0.9841)\n",
      "torch.Size([32, 5, 1])\n",
      "tensor(0.9837)\n",
      "torch.Size([32, 4, 1])\n",
      "tensor(0.9840)\n",
      "torch.Size([32, 3, 1])\n",
      "tensor(0.9842)\n",
      "torch.Size([32, 2, 1])\n",
      "tensor(0.9837)\n",
      "torch.Size([32, 1, 1])\n",
      "tensor(0.9835)\n"
     ]
    }
   ],
   "source": [
    "for batch in range(len(train_p[0])):\n",
    "    # load x, y from batch\n",
    "    entry_x, entry_y = train_p[0][batch], train_p[1][batch]\n",
    "    # sepeate data from sentence lengths\n",
    "    y_outs, y_seqs = entry_y\n",
    "    \n",
    "    # get y_length\n",
    "    y_len = len(y_outs[0])\n",
    "    \n",
    "    num_classes = weightshape[0]\n",
    "    batch_size = y_outs.shape[0]\n",
    "    \n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "    # iterate through the words in y\n",
    "    for w in range(y_len):\n",
    "        # get indexes of future words\n",
    "        labels = y_outs[:,w:].long().unsqueeze(2)\n",
    "        print(labels.shape)\n",
    "        # set up bag of words container\n",
    "        bow = torch.FloatTensor(batch_size, y_len-w, num_classes).zero_()\n",
    "        # create SBOW\n",
    "        bow.scatter_(2, labels, 1)\n",
    "        # sum by dimension and limit values between 0 and 1\n",
    "        bow = torch.sum(bow, dim=1).clamp(0,1)\n",
    "#         print(bow[0])\n",
    "#         print(\"BOW:\",bow.shape)\n",
    "        \n",
    "        # generate some random matrix of the same shape\n",
    "        guess = torch.rand(batch_size, num_classes)\n",
    "#         print(\"GUESS\",guess.shape)\n",
    "#         crit = F.cross_entropy(ignore_index=word2id['pad'])\n",
    "        err = loss(guess,bow)\n",
    "        print(err)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n",
      "torch.Size([2, 3, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0., 1.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 0.]],\n",
       "\n",
       "        [[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [1., 0., 0.]]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = 3\n",
    "labels = torch.LongTensor([[[2,1,0]], [[0,1,0]]]).permute(0,2,1) # Let this be your current batch\n",
    "print(labels.shape)\n",
    "batch_size, k, _ = labels.size()\n",
    "labels_one_hot = torch.FloatTensor(batch_size, k, num_classes).zero_()\n",
    "print(labels_one_hot.shape)\n",
    "labels_one_hot.scatter_(2, labels, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b3698371c448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "torch.zeros(3, 5).scatter_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qwer = torch.tensor([[2], [3]])\n",
    "print(qwer.shape)\n",
    "z = torch.zeros(2, 4)\n",
    "print(z.shape)\n",
    "z.scatter_(1, qwer, 1.23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
